{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class My_LSTM_cell(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A simple LSTM cell network for educational AI-summer purposes\n",
    "    \"\"\"\n",
    "    def __init__(self, input_length=10, hidden_length=20):\n",
    "        super(My_LSTM_cell, self).__init__()\n",
    "        self.input_length = input_length\n",
    "        self.hidden_length = hidden_length\n",
    "\n",
    "        # forget gate components\n",
    "        # 1. DEFINE FORGET GATE COMPONENTS\n",
    "        self.linear_forgot_w1 = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
    "        self.linear_forgot_r1 = nn.Linear(self.hidden_length, self.hidden_length, bias=False)\n",
    "        self.sigmoid_forgot = nn.Sigmoid()\n",
    "\n",
    "        # input gate components\n",
    "        self.linear_input_w2 = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
    "        self.linear_input_r2 = nn.Linear(self.hidden_length, self.hidden_length, bias=False)\n",
    "        self.sigmoid_input = nn.Sigmoid()\n",
    "\n",
    "        # cell memory components\n",
    "        # 2. DEFINE CELL MEMORY COMPONENTS\n",
    "        self.linear_memory_w3 = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
    "        self.linear_memory_r3 = nn.Linear(self.hidden_length, self.hidden_length, bias=False)\n",
    "        self.activation_memory = nn.Tanh()\n",
    "\n",
    "        # out gate components\n",
    "        # 3. DEFINE OUT GATE COMPONENTS\n",
    "        self.linear_out_w4 = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
    "        self.linear_out_r4 = nn.Linear(self.hidden_length, self.hidden_length, bias=False)\n",
    "        self.sigmoid_out = nn.Sigmoid()\n",
    "\n",
    "        # final output\n",
    "        # 4. DEFINE OUTPUT\n",
    "        self.activation_final = nn.Tanh() \n",
    "\n",
    "    def forget(self, x, h):\n",
    "        # 5. FORGET GATE\n",
    "        x_f = self.linear_forgot_w1(x)\n",
    "        h_f = self.linear_forgot_r1(h)\n",
    "        return self.sigmoid_forgot(x_f + h_f)\n",
    "      \n",
    "\n",
    "    def input_gate(self, x, h):\n",
    "\n",
    "        # input gate\n",
    "        x_i = self.linear_input_w2(x)\n",
    "        h_i = self.linear_input_r2(h)\n",
    "        i = self.sigmoid_input(x_i + h_i)\n",
    "        return i\n",
    "\n",
    "    def cell_memory_gate(self, i, f, x, h, c_prev):\n",
    "        # 6. CELL MEMORY GATE\n",
    "        x_c = self.linear_memory_w3(x)\n",
    "        h_c = self.linear_memory_r3(h)\n",
    "        \n",
    "        return f*c_prev + i * self.activation_memory(x_c + h_c)\n",
    "       \n",
    "\n",
    "    def out_gate(self, x, h):\n",
    "        # 7. OUT GATE\n",
    "        x_o = self.linear_out_w4(x)\n",
    "        h_o = self.linear_out_r4(h)\n",
    "        \n",
    "        return self.sigmoid_out(x_o + h_o)\n",
    "       \n",
    "    def forward(self, x, tuple_in ):\n",
    "        (h, c_prev) = tuple_in\n",
    "        # Equation 1. input gate\n",
    "        i = self.input_gate(x, h)\n",
    "\n",
    "        # Equation 2. forget gate\n",
    "        f = self.forget(x, h)\n",
    "\n",
    "        # Equation 3. updating the cell memory\n",
    "        c_next = self.cell_memory_gate(i, f, x, h,c_prev)\n",
    "\n",
    "        # Equation 4. calculate the main output gate\n",
    "        o = self.out_gate(x, h)\n",
    "\n",
    "        # Equation 5. produce next hidden output\n",
    "        h_next = o * self.activation_final(c_next)\n",
    "\n",
    "        return h_next, c_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LSTM (Sine Wave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "#set seed to be able to replicate the resutls\n",
    "seed = 172\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "def generate_sin_wave_data():\n",
    "    T = 20\n",
    "    L = 1000\n",
    "    N = 200\n",
    "\n",
    "    x = np.empty((N, L), 'int64')\n",
    "    x[:] = np.array(range(L)) + np.random.randint(-4 * T, 4 * T, N).reshape(N, 1)\n",
    "    data = np.sin(x / 1.0 / T).astype('float64')\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model consists of two LSTM cells as we already mentioned. The first cell receives an input of length 1 and has an output of length 51 while the second one receives an input of length 51 and has an output of length 1.\n",
    "\n",
    "Have a closer look in the `forward` method. Did you noticed that we can generate future predictions? The first for-loop runs on all data points in the input data. The second for loop recieves thet last data point and tries to generate new ones for the next time step.\n",
    "\n",
    "For each training epoch, we train the model and then we generate 1000 new data points.\n",
    "\n",
    "After each epoch, we plot the predicted data points to visualize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP:  0\n",
      "loss: 0.5014640293663556\n",
      "loss: 0.49865314833625013\n",
      "loss: 0.19580064637837746\n",
      "loss: 0.3981928283278413\n",
      "loss: 0.03893307468453568\n",
      "loss: 0.031036286462091025\n",
      "loss: 0.026284214629450284\n",
      "loss: 0.025777950984984557\n",
      "loss: 0.025162068537741427\n",
      "loss: 0.019594104597675854\n",
      "loss: 0.013388312694249952\n",
      "loss: 0.008929125547073905\n",
      "loss: 0.006269382987125089\n",
      "loss: 0.004663699764098029\n",
      "loss: 0.002585471178688211\n",
      "loss: 0.0010907439706587241\n",
      "loss: 0.0006870894258155633\n",
      "loss: 0.0005395663233956395\n",
      "loss: 0.00046112457580549816\n",
      "loss: 0.000448608219614525\n",
      "test loss: 0.0005382116290604778\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class Sequence(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sequence, self).__init__()\n",
    "\n",
    "        self.rnn1 = nn.LSTMCell(1, 51)\n",
    "        self.rnn2 = nn.LSTMCell(51, 51)\n",
    "\n",
    "        self.linear = nn.Linear(51, 1)\n",
    "\n",
    "    def forward(self, input, future=0):\n",
    "        outputs = []\n",
    "        h_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "        c_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "        h_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "        c_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "\n",
    "        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
    "\n",
    "            h_t, c_t = self.rnn1(input_t, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.rnn2(h_t, (h_t2, c_t2))\n",
    "\n",
    "\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "\n",
    "        # if we should predict the future\n",
    "        for i in range(future):\n",
    "\n",
    "            h_t, c_t = self.rnn1(input_t, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.rnn2(h_t, (h_t2, c_t2))\n",
    "\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "\n",
    "        outputs = torch.stack(outputs, 1).squeeze(2)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def train():\n",
    "    # load data and make training set\n",
    "    data = generate_sin_wave_data()\n",
    "    input = torch.from_numpy(data[3:, :-1])\n",
    "    target = torch.from_numpy(data[3:, 1:])\n",
    "    test_input = torch.from_numpy(data[:3, :-1])\n",
    "    test_target = torch.from_numpy(data[:3, 1:])\n",
    "\n",
    "    seq = Sequence()\n",
    "\n",
    "    seq.double()\n",
    "    criterion = nn.MSELoss()\n",
    "    # use LBFGS as optimizer since we can load the whole data to train\n",
    "    optimizer = optim.LBFGS(seq.parameters(), lr=0.8)\n",
    "    \n",
    "    # begin to train\n",
    "    for i in range(1):\n",
    "        print('STEP: ', i)\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            out = seq(input)\n",
    "            loss = criterion(out, target)\n",
    "            print('loss:', loss.item())\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "        \n",
    "        # begin to predict, no need to track gradient here\n",
    "        with torch.no_grad():\n",
    "            future = 1000\n",
    "            pred = seq(test_input, future=future)\n",
    "            loss = criterion(pred[:, :-future], test_target)\n",
    "            print('test loss:', loss.item())\n",
    "            y = pred.detach().numpy()\n",
    "            \n",
    "        # draw the result\n",
    "        plt.figure(figsize=(30, 10))\n",
    "        plt.title('Predict future values for time sequences\\n(Dashlines are predicted values)', fontsize=30)\n",
    "        plt.xlabel('x', fontsize=20)\n",
    "        plt.ylabel('y', fontsize=20)\n",
    "        plt.xticks(fontsize=20)\n",
    "        plt.yticks(fontsize=20)\n",
    "\n",
    "        def draw(yi, color):\n",
    "            plt.plot(np.arange(input.size(1)), yi[:input.size(1)], color, linewidth=2.0)\n",
    "            plt.plot(np.arange(input.size(1), input.size(1) + future), yi[input.size(1):], color + ':', linewidth=2.0)\n",
    "\n",
    "        draw(y[0], 'r')\n",
    "        draw(y[1], 'g')\n",
    "        draw(y[2], 'b')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    generate_sin_wave_data()\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
