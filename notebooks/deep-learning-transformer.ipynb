{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechaninsm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "seed = 172\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, y_dim: int, h_dim: int):\n",
    "        super().__init__()\n",
    "        #1. Define the vector dimensions and the trainable parameters\n",
    "        self.W = nn.Parameter(torch.FloatTensor(y_dim, h_dim))\n",
    "\n",
    "    def forward(self,\n",
    "                y: torch.Tensor, # y.size() = (1, y_dim)\n",
    "                h: torch.Tensor, # h.size() = (1, h_dim)\n",
    "                ):\n",
    "        #2. Define the forward pass\n",
    "        \n",
    "        score = torch.matmul(torch.matmul(y,self.W),h.T)\n",
    "        alpha = F.softmax(score)\n",
    "        \n",
    "        z = torch.matmul(alpha, h)\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MHA (Multi Head Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads: int, d_model: int, dropout: float = 0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.h = n_heads\n",
    "\n",
    "        self.linears = nn.ModuleList(\n",
    "            [copy.deepcopy(nn.Linear(d_model, d_model)) for _ in range(4)]\n",
    "        )\n",
    "        self.sdpa = ScaledDotProductAttention()\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query: torch.FloatTensor, \n",
    "                key: torch.FloatTensor, \n",
    "                value: torch.FloatTensor,\n",
    "                mask: Optional[torch.ByteTensor] = None\n",
    "                ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            `query`: shape (batch_size, max_len, d_model)\n",
    "            `key`: shape (batch_size, max_len, d_model)\n",
    "            `value`: shape (batch_size, max_len, d_model)\n",
    "            `mask`: shape (batch_size, max_len)\n",
    "\n",
    "        Returns:\n",
    "            shape (batch_size, max_len, d_model)\n",
    "        \"\"\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads. B*1*1*L\n",
    "            mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "        \n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        # x: B x H x L x D_v\n",
    "        x, self.attn = self.sdpa(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            `d_model`: model dimension\n",
    "            `d_ff`: hidden dimension of feed forward layer\n",
    "            `dropout`: ropout rate, default 0.1\n",
    "        \"\"\"\n",
    "        super(FeedForward, self).__init__() \n",
    "        ## 1. DEFINE 2 LINEAR LAYERS AND DROPOUT HERE\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "       \n",
    "       \n",
    "\n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            `x`: shape (batch_size, max_len, d_model)\n",
    "        Returns:\n",
    "            same shape as input x\n",
    "        \"\"\"\n",
    "        ## 2.  RETURN THE FORWARD PASS \n",
    "        return self.layer(x)\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features: int, eps: float = 1e-6):\n",
    "        # features = d_model\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a = nn.Parameter(torch.ones(features))\n",
    "        self.b = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a * (x - mean) / (std + self.eps) + self.b\n",
    "    \n",
    "\n",
    "class SkipConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size: int, dropout: float):\n",
    "        super(SkipConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, \n",
    "                x: torch.FloatTensor, \n",
    "                sublayer: Union[MultiHeadAttention, FeedForward]\n",
    "                ) -> torch.FloatTensor:\n",
    "        \"\"\"Apply residual connection to any sublayer with the same size.\"\"\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "    \n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Encoder is made up of self-attn and feed forward\"\"\"\n",
    "\n",
    "    def __init__(self, size: int, self_attn: MultiHeadAttention, feed_forward: FeedForward, dropout: float):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = nn.ModuleList([copy.deepcopy(SkipConnection(size, dropout)) for _ in range(2)])\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor, mask: torch.ByteTensor) -> torch.FloatTensor:\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Core encoder is a stack of N layers\"\"\"\n",
    "\n",
    "    def __init__(self, layer: EncoderLayer, N: int):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor, mask: torch.ByteTensor) -> torch.FloatTensor:\n",
    "        \"\"\"Pass the input (and mask) through each layer in turn.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"The encoder of transformer\n",
    "    Args:\n",
    "        `n_layers`: number of stacked encoder layers\n",
    "        `d_model`: model dimension\n",
    "        `d_ff`: hidden dimension of feed forward layer\n",
    "        `n_heads`: number of heads of self-attention\n",
    "        `dropout`: dropout rate, default 0.1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, n_heads: int = 1, n_layers: int = 1,\n",
    "                 dropout: float = 0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.multi_headed_attention = MultiHeadAttention(n_heads, d_model, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.encoder_layer = EncoderLayer(d_model, self.multi_headed_attention, self.feed_forward, dropout)\n",
    "        self.encoder = Encoder(self.encoder_layer, n_layers)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor, mask: torch.ByteTensor) -> torch.FloatTensor:\n",
    "        return self.encoder(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
